{
  "cells": [
    {
      "attachments": {
        "train_accuracy.png": {
          "image/png": "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"
        },
        "valid_accuracy.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {
        "id": "zC5KwRyl6Flp"
      },
      "source": [
        "### Review\n",
        "- 调参很顺利，一次就过了strong baseline\n",
        "- ![train_accuracy.png](attachment:train_accuracy.png)\n",
        "- ![valid_accuracy.png](attachment:valid_accuracy.png)\n",
        "## Tips\n",
        "- optimizer使用warmup\n",
        "- 与其自己胡乱写框架，不如学习一些论文中的框架经验，比如应该设几个self-attention层，layer的宽度最好怎么设置，input和output的大小最好呈怎样的关系\n",
        "- 这次训练时间拉长到了5h，可以看到训练时间不够长的话model的能力就没有充分发挥出来\n",
        "- tensorboard的使用要注意命令的路径"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task description\n",
        "- Classify the speakers of given features.\n",
        "- Main goal: Learn how to use transformer.\n",
        "- Baselines:\n",
        "  - Easy: Run sample code and know how to use transformer.\n",
        "  - Medium: Know how to adjust parameters of transformer.\n",
        "  - Hard: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer.\n",
        "\n",
        "- Other links\n",
        "  - Kaggle: [link](https://www.kaggle.com/t/859c9ca9ede14fdea841be627c412322)\n",
        "  - Slide: [link](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW04/HW04.pdf)\n",
        "  - Data: [link](https://drive.google.com/file/d/1T0RPnu-Sg5eIPwQPfYysipfcz81MnsYe/view?usp=sharing)\n",
        "  - Video (Chinese): [link](https://www.youtube.com/watch?v=EPerg2UnGaI)\n",
        "  - Video (English): [link](https://www.youtube.com/watch?v=Gpz6AUvCak0)\n",
        "  - Solution for downloading dataset fail.: [link](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPDoreyypeJE"
      },
      "source": [
        "# Download dataset\n",
        "- Please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing) to download data\n",
        "- Data is [here](https://drive.google.com/file/d/1gaFy8RaQVUEXo2n0peCBR5gYKCB-mNHc/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QvpaILXnJIcw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--id] [--proxy PROXY]\n",
            "             [--speed SPEED] [--no-cookies] [--no-check-certificate]\n",
            "             [--continue] [--folder] [--remaining-ok] [--format FORMAT]\n",
            "             [--user-agent USER_AGENT]\n",
            "             url_or_id\n",
            "gdown: error: unrecognized arguments: your own data download link'\n",
            "'unzip' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 'paste your own data download link' --output Dataset.zip\n",
        "!unzip Dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1gYr_aoNDue"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz_NpuAipk3h"
      },
      "source": [
        "## Dataset\n",
        "- Original dataset is [Voxceleb1](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/).\n",
        "- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb1.\n",
        "- We randomly select 600 speakers from Voxceleb1.\n",
        "- Then preprocess the raw waveforms into mel-spectrograms.\n",
        "\n",
        "- Args:\n",
        "  - data_dir: The path to the data directory.\n",
        "  - metadata_path: The path to the metadata.\n",
        "  - segment_len: The length of audio segment for training.\n",
        "- The architecture of data directory \\\\\n",
        "  - data directory \\\\\n",
        "  |---- metadata.json \\\\\n",
        "  |---- testdata.json \\\\\n",
        "  |---- mapping.json \\\\\n",
        "  |---- uttr-{random string}.pt \\\\\n",
        "\n",
        "- The information in metadata\n",
        "  - \"n_mels\": The dimention of mel-spectrogram.\n",
        "  - \"speakers\": A dictionary.\n",
        "    - Key: speaker ids.\n",
        "    - value: \"feature_path\" and \"mel_len\"\n",
        "\n",
        "\n",
        "For efficiency, we segment the mel-spectrograms into segments in the traing step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cd7hoGhYtbXQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self, data_dir, segment_len=128):\n",
        "    self.data_dir = data_dir\n",
        "    self.segment_len = segment_len\n",
        "\n",
        "    # Load the mapping from speaker neme to their corresponding id.\n",
        "    mapping_path = Path(data_dir) / \"mapping.json\"\n",
        "    mapping = json.load(mapping_path.open())\n",
        "    self.speaker2id = mapping[\"speaker2id\"]\n",
        "\n",
        "    # Load metadata of training data.\n",
        "    metadata_path = Path(data_dir) / \"metadata.json\"\n",
        "    metadata = json.load(open(metadata_path))[\"speakers\"]\n",
        "\n",
        "    # Get the total number of speaker.\n",
        "    self.speaker_num = len(metadata.keys())\n",
        "    self.data = []\n",
        "    for speaker in metadata.keys():\n",
        "      for utterances in metadata[speaker]:\n",
        "        self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    feat_path, speaker = self.data[index]\n",
        "    # Load preprocessed mel-spectrogram.\n",
        "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
        "\n",
        "    # Segmemt mel-spectrogram into \"segment_len\" frames.\n",
        "    if len(mel) > self.segment_len:\n",
        "      # Randomly get the starting point of the segment.\n",
        "      start = random.randint(0, len(mel) - self.segment_len)\n",
        "      # Get a segment with \"segment_len\" frames.\n",
        "      mel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
        "    else:\n",
        "      mel = torch.FloatTensor(mel)\n",
        "    # Turn the speaker id into long for computing loss later.\n",
        "    speaker = torch.FloatTensor([speaker]).long()\n",
        "    return mel, speaker\n",
        "\n",
        "  def get_speaker_number(self):\n",
        "    return self.speaker_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqJxjoi_NGnB"
      },
      "source": [
        "## Dataloader\n",
        "- Split dataset into training dataset(90%) and validation dataset(10%).\n",
        "- Create dataloader to iterate the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zuT1AuFENI8t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "  # Process features within a batch.\n",
        "  \"\"\"Collate a batch of data.\"\"\"\n",
        "  mel, speaker = zip(*batch)\n",
        "  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n",
        "  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n",
        "  # mel: (batch size, length, 40)\n",
        "  return mel, torch.FloatTensor(speaker).long()\n",
        "\n",
        "\n",
        "def get_dataloader(data_dir, batch_size, n_workers):\n",
        "  \"\"\"Generate dataloader\"\"\"\n",
        "  dataset = myDataset(data_dir)\n",
        "  speaker_num = dataset.get_speaker_number()\n",
        "  # Split dataset into training dataset and validation dataset\n",
        "  trainlen = int(0.9 * len(dataset))\n",
        "  print(f\"[Info]: Training set length: {trainlen}, Validation set length: {len(dataset) - trainlen}\", flush=True)\n",
        "  lengths = [trainlen, len(dataset) - trainlen]\n",
        "  trainset, validset = random_split(dataset, lengths)\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "    trainset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=n_workers,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_batch,\n",
        "  )\n",
        "  valid_loader = DataLoader(\n",
        "    validset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=n_workers,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_batch,\n",
        "  )\n",
        "\n",
        "  return train_loader, valid_loader, speaker_num\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0x6eXiHpr4R"
      },
      "source": [
        "# Model\n",
        "- TransformerEncoderLayer:\n",
        "  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "  - Parameters:\n",
        "    - d_model: the number of expected features of the input (required).\n",
        "\n",
        "    - nhead: the number of heads of the multiheadattention models (required).\n",
        "\n",
        "    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "\n",
        "    - dropout: the dropout value (default=0.1).\n",
        "\n",
        "    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "\n",
        "- TransformerEncoder:\n",
        "  - TransformerEncoder is a stack of N transformer encoder layers\n",
        "  - Parameters:\n",
        "    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
        "\n",
        "    - num_layers: the number of sub-encoder-layers in the encoder (required).\n",
        "\n",
        "    - norm: the layer normalization component (optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SHX4eVj4tjtd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, d_model=128, n_spks=600, dropout=0.1):\n",
        "    super().__init__()\n",
        "    # Project the dimension of features from that of input into d_model.\n",
        "    self.prenet = nn.Sequential(\n",
        "            nn.Linear(40, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, d_model),\n",
        "        )\n",
        "    # TODO:\n",
        "    #   Change Transformer to Conformer.\n",
        "    #   https://arxiv.org/abs/2005.08100\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "      d_model=d_model, dim_feedforward=512, nhead=8\n",
        "    )\n",
        "    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
        "\n",
        "    # Project the the dimension of features from d_model into speaker nums.\n",
        "    self.pred_layer = nn.Sequential(\n",
        "      nn.Linear(d_model, d_model),\n",
        "      nn.BatchNorm1d(d_model),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(d_model, n_spks),\n",
        "    )\n",
        "\n",
        "  def forward(self, mels):\n",
        "    \"\"\"\n",
        "    args:\n",
        "      mels: (batch size, length, 40)\n",
        "    return:\n",
        "      out: (batch size, n_spks)\n",
        "    \"\"\"\n",
        "    # out: (batch size, length, d_model)\n",
        "    out = self.prenet(mels)\n",
        "    # out: (length, batch size, d_model)\n",
        "    out = out.permute(1, 0, 2)\n",
        "    # The encoder layer expect features in the shape of (length, batch size, d_model).\n",
        "    out = self.encoder(out)\n",
        "    # out: (batch size, length, d_model)\n",
        "    out = out.transpose(0, 1)\n",
        "    # mean pooling\n",
        "    stats = out.mean(dim=1)\n",
        "\n",
        "    # out: (batch, n_spks)\n",
        "    out = self.pred_layer(stats)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__DolPGpvDZ"
      },
      "source": [
        "# Learning rate schedule\n",
        "- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n",
        "- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n",
        "- The warmup schedule\n",
        "  - Set learning rate to 0 in the beginning.\n",
        "  - The learning rate increases linearly from 0 to initial learning rate during warmup period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K-0816BntqT9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(\n",
        "  optimizer: Optimizer,\n",
        "  num_warmup_steps: int,\n",
        "  num_training_steps: int,\n",
        "  num_cycles: float = 0.5,\n",
        "  last_epoch: int = -1,\n",
        "):\n",
        "  \"\"\"\n",
        "  Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
        "  initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n",
        "  initial lr set in the optimizer.\n",
        "\n",
        "  Args:\n",
        "    optimizer (:class:`~torch.optim.Optimizer`):\n",
        "      The optimizer for which to schedule the learning rate.\n",
        "    num_warmup_steps (:obj:`int`):\n",
        "      The number of steps for the warmup phase.\n",
        "    num_training_steps (:obj:`int`):\n",
        "      The total number of training steps.\n",
        "    num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n",
        "      The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n",
        "      following a half-cosine).\n",
        "    last_epoch (:obj:`int`, `optional`, defaults to -1):\n",
        "      The index of the last epoch when resuming training.\n",
        "\n",
        "  Return:\n",
        "    :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
        "  \"\"\"\n",
        "\n",
        "  def lr_lambda(current_step):\n",
        "    # Warmup\n",
        "    if current_step < num_warmup_steps:\n",
        "      return float(current_step) / float(max(1, num_warmup_steps))\n",
        "    # decadence\n",
        "    progress = float(current_step - num_warmup_steps) / float(\n",
        "      max(1, num_training_steps - num_warmup_steps)\n",
        "    )\n",
        "    return max(\n",
        "      0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
        "    )\n",
        "\n",
        "  return LambdaLR(optimizer, lr_lambda, last_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP03FFo9K8DS"
      },
      "source": [
        "# Model Function\n",
        "- Model forward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fohaLEFJK9-t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def model_fn(batch, model, criterion, device):\n",
        "  \"\"\"Forward a batch through the model.\"\"\"\n",
        "\n",
        "  mels, labels = batch\n",
        "  mels = mels.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  outs = model(mels)\n",
        "\n",
        "  loss = criterion(outs, labels)\n",
        "\n",
        "  # Get the speaker id with highest probability.\n",
        "  preds = outs.argmax(1)\n",
        "  # Compute accuracy.\n",
        "  accuracy = torch.mean((preds == labels).float())\n",
        "\n",
        "  return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7cg-YrzLQcf"
      },
      "source": [
        "# Validate\n",
        "- Calculate accuracy of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mD-_p6nWLO2L"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "\n",
        "def valid(dataloader, model, criterion, device):\n",
        "  \"\"\"Validate on validation set.\"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  running_accuracy = 0.0\n",
        "  pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n",
        "\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    with torch.no_grad():\n",
        "      loss, accuracy = model_fn(batch, model, criterion, device)\n",
        "      running_loss += loss.item()\n",
        "      running_accuracy += accuracy.item()\n",
        "\n",
        "    pbar.update(dataloader.batch_size)\n",
        "    pbar.set_postfix(\n",
        "      loss=f\"{running_loss / (i+1):.2f}\",\n",
        "      accuracy=f\"{running_accuracy / (i+1):.2f}\",\n",
        "    )\n",
        "\n",
        "  pbar.close()\n",
        "  model.train()\n",
        "\n",
        "  return running_accuracy / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHXyal5p1W5"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "chRQE7oYtw62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Info]: Use cuda now!\n",
            "[Info]: Training set length: 62494, Validation set length: 6944\n",
            "[Info]: Finish loading data!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Info]: Finish creating model!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:   0% 0/500 [00:00<?, ? step/s]C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_22356\\97271689.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
            "Train: 100% 500/500 [01:52<00:00,  4.43 step/s, accuracy=0.25, loss=3.41, step=500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 931.71 uttr/s, accuracy=0.28, loss=3.44] \n",
            "Train: 100% 500/500 [01:51<00:00,  4.47 step/s, accuracy=0.47, loss=2.40, step=1000]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 993.71 uttr/s, accuracy=0.47, loss=2.34] \n",
            "Train: 100% 500/500 [01:45<00:00,  4.72 step/s, accuracy=0.56, loss=1.97, step=1500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 977.41 uttr/s, accuracy=0.58, loss=1.82] \n",
            "Train: 100% 500/500 [02:07<00:00,  3.91 step/s, accuracy=0.63, loss=1.62, step=2000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 826.38 uttr/s, accuracy=0.62, loss=1.64]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2000, best model saved. (accuracy=0.6188)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:06<00:00,  3.94 step/s, accuracy=0.75, loss=1.13, step=2500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 665.14 uttr/s, accuracy=0.65, loss=1.46]\n",
            "Train: 100% 500/500 [02:02<00:00,  4.07 step/s, accuracy=0.70, loss=1.23, step=3000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 804.10 uttr/s, accuracy=0.69, loss=1.29]\n",
            "Train: 100% 500/500 [02:00<00:00,  4.14 step/s, accuracy=0.73, loss=1.06, step=3500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 669.92 uttr/s, accuracy=0.70, loss=1.24]\n",
            "Train: 100% 500/500 [02:03<00:00,  4.04 step/s, accuracy=0.76, loss=0.95, step=4000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 791.29 uttr/s, accuracy=0.71, loss=1.20]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4000, best model saved. (accuracy=0.7121)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:02<00:00,  4.10 step/s, accuracy=0.72, loss=0.98, step=4500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 777.06 uttr/s, accuracy=0.73, loss=1.13]\n",
            "Train: 100% 500/500 [02:01<00:00,  4.11 step/s, accuracy=0.73, loss=1.05, step=5000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 803.09 uttr/s, accuracy=0.74, loss=1.09]\n",
            "Train: 100% 500/500 [01:58<00:00,  4.21 step/s, accuracy=0.75, loss=0.88, step=5500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 795.46 uttr/s, accuracy=0.75, loss=1.04] \n",
            "Train: 100% 500/500 [02:00<00:00,  4.16 step/s, accuracy=0.81, loss=0.81, step=6000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 823.15 uttr/s, accuracy=0.76, loss=1.04]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 6000, best model saved. (accuracy=0.7554)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:03<00:00,  4.03 step/s, accuracy=0.81, loss=0.81, step=6500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 960.93 uttr/s, accuracy=0.76, loss=1.01] \n",
            "Train: 100% 500/500 [02:19<00:00,  3.58 step/s, accuracy=0.83, loss=0.63, step=7000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 660.56 uttr/s, accuracy=0.77, loss=0.97]\n",
            "Train: 100% 500/500 [02:28<00:00,  3.38 step/s, accuracy=0.81, loss=0.67, step=7500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 697.24 uttr/s, accuracy=0.78, loss=0.97]\n",
            "Train: 100% 500/500 [02:26<00:00,  3.42 step/s, accuracy=0.81, loss=0.66, step=8000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 724.16 uttr/s, accuracy=0.79, loss=0.92]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 8000, best model saved. (accuracy=0.7872)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:22<00:00,  3.52 step/s, accuracy=0.84, loss=0.71, step=8500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 710.30 uttr/s, accuracy=0.78, loss=0.95]\n",
            "Train: 100% 500/500 [02:27<00:00,  3.39 step/s, accuracy=0.82, loss=0.71, step=9000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 735.46 uttr/s, accuracy=0.79, loss=0.92]\n",
            "Train: 100% 500/500 [02:24<00:00,  3.47 step/s, accuracy=0.82, loss=0.65, step=9500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 735.21 uttr/s, accuracy=0.80, loss=0.89]\n",
            "Train: 100% 500/500 [02:13<00:00,  3.74 step/s, accuracy=0.82, loss=0.70, step=1e+4]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 770.64 uttr/s, accuracy=0.80, loss=0.89]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 10000, best model saved. (accuracy=0.8014)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:17<00:00,  3.64 step/s, accuracy=0.81, loss=0.71, step=10500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 749.43 uttr/s, accuracy=0.79, loss=0.92]\n",
            "Train: 100% 500/500 [02:18<00:00,  3.62 step/s, accuracy=0.88, loss=0.48, step=11000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 690.75 uttr/s, accuracy=0.81, loss=0.85]\n",
            "Train: 100% 500/500 [02:17<00:00,  3.63 step/s, accuracy=0.87, loss=0.45, step=11500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 718.12 uttr/s, accuracy=0.80, loss=0.87]\n",
            "Train: 100% 500/500 [02:21<00:00,  3.54 step/s, accuracy=0.89, loss=0.52, step=12000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 687.36 uttr/s, accuracy=0.80, loss=0.88]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 12000, best model saved. (accuracy=0.8086)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:07<00:00,  3.91 step/s, accuracy=0.84, loss=0.55, step=12500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 754.95 uttr/s, accuracy=0.82, loss=0.82] \n",
            "Train: 100% 500/500 [02:10<00:00,  3.83 step/s, accuracy=0.82, loss=0.64, step=13000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 696.28 uttr/s, accuracy=0.82, loss=0.82]\n",
            "Train: 100% 500/500 [02:13<00:00,  3.75 step/s, accuracy=0.88, loss=0.50, step=13500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 636.91 uttr/s, accuracy=0.82, loss=0.81]\n",
            "Train: 100% 500/500 [02:13<00:00,  3.73 step/s, accuracy=0.85, loss=0.54, step=14000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 678.45 uttr/s, accuracy=0.82, loss=0.81]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 14000, best model saved. (accuracy=0.8215)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:13<00:00,  3.75 step/s, accuracy=0.88, loss=0.43, step=14500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 634.35 uttr/s, accuracy=0.82, loss=0.82]\n",
            "Train: 100% 500/500 [02:09<00:00,  3.87 step/s, accuracy=0.85, loss=0.40, step=15000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 647.54 uttr/s, accuracy=0.83, loss=0.79]\n",
            "Train: 100% 500/500 [02:15<00:00,  3.68 step/s, accuracy=0.88, loss=0.40, step=15500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 673.77 uttr/s, accuracy=0.81, loss=0.83]\n",
            "Train: 100% 500/500 [01:49<00:00,  4.58 step/s, accuracy=0.87, loss=0.46, step=16000]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 988.74 uttr/s, accuracy=0.82, loss=0.81] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 16000, best model saved. (accuracy=0.8261)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:38<00:00,  5.08 step/s, accuracy=0.88, loss=0.40, step=16500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 972.88 uttr/s, accuracy=0.82, loss=0.84] \n",
            "Train: 100% 500/500 [01:38<00:00,  5.05 step/s, accuracy=0.91, loss=0.37, step=17000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 979.49 uttr/s, accuracy=0.83, loss=0.80] \n",
            "Train: 100% 500/500 [01:39<00:00,  5.05 step/s, accuracy=0.89, loss=0.41, step=17500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 976.38 uttr/s, accuracy=0.82, loss=0.83] \n",
            "Train: 100% 500/500 [01:38<00:00,  5.09 step/s, accuracy=0.88, loss=0.37, step=18000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 979.14 uttr/s, accuracy=0.82, loss=0.83] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 18000, best model saved. (accuracy=0.8264)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:38<00:00,  5.09 step/s, accuracy=0.89, loss=0.37, step=18500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 969.95 uttr/s, accuracy=0.83, loss=0.80] \n",
            "Train: 100% 500/500 [01:39<00:00,  5.04 step/s, accuracy=0.89, loss=0.37, step=19000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 969.43 uttr/s, accuracy=0.83, loss=0.81] \n",
            "Train: 100% 500/500 [01:37<00:00,  5.12 step/s, accuracy=0.89, loss=0.40, step=19500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 957.10 uttr/s, accuracy=0.82, loss=0.79] \n",
            "Train: 100% 500/500 [01:37<00:00,  5.14 step/s, accuracy=0.91, loss=0.37, step=2e+4] \n",
            "Valid: 100% 6912/6944 [00:07<00:00, 939.75 uttr/s, accuracy=0.83, loss=0.81] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20000, best model saved. (accuracy=0.8267)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:38<00:00,  5.06 step/s, accuracy=0.91, loss=0.34, step=20500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 956.55 uttr/s, accuracy=0.83, loss=0.79] \n",
            "Train: 100% 500/500 [01:37<00:00,  5.15 step/s, accuracy=0.89, loss=0.36, step=21000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 970.35 uttr/s, accuracy=0.84, loss=0.76] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.22 step/s, accuracy=0.86, loss=0.49, step=21500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 945.39 uttr/s, accuracy=0.83, loss=0.80] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.22 step/s, accuracy=0.93, loss=0.23, step=22000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 936.20 uttr/s, accuracy=0.85, loss=0.71] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 22000, best model saved. (accuracy=0.8487)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:35<00:00,  5.24 step/s, accuracy=0.93, loss=0.30, step=22500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 968.93 uttr/s, accuracy=0.84, loss=0.73] \n",
            "Train: 100% 500/500 [01:34<00:00,  5.28 step/s, accuracy=0.91, loss=0.39, step=23000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 957.45 uttr/s, accuracy=0.84, loss=0.75] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.24 step/s, accuracy=0.93, loss=0.31, step=23500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 929.32 uttr/s, accuracy=0.85, loss=0.75] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.18 step/s, accuracy=0.90, loss=0.31, step=24000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 966.68 uttr/s, accuracy=0.85, loss=0.73] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 24000, best model saved. (accuracy=0.8516)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:34<00:00,  5.28 step/s, accuracy=0.90, loss=0.28, step=24500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 963.77 uttr/s, accuracy=0.84, loss=0.78] \n",
            "Train: 100% 500/500 [01:33<00:00,  5.33 step/s, accuracy=0.90, loss=0.34, step=25000]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 1011.06 uttr/s, accuracy=0.84, loss=0.77]\n",
            "Train: 100% 500/500 [01:35<00:00,  5.25 step/s, accuracy=0.93, loss=0.26, step=25500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 970.66 uttr/s, accuracy=0.85, loss=0.75] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.16 step/s, accuracy=0.91, loss=0.35, step=26000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 985.82 uttr/s, accuracy=0.85, loss=0.73] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 26000, best model saved. (accuracy=0.8516)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:37<00:00,  5.14 step/s, accuracy=0.93, loss=0.28, step=26500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 838.93 uttr/s, accuracy=0.85, loss=0.76] \n",
            "Train: 100% 500/500 [02:03<00:00,  4.05 step/s, accuracy=0.91, loss=0.29, step=27000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 733.37 uttr/s, accuracy=0.85, loss=0.77]\n",
            "Train: 100% 500/500 [02:05<00:00,  3.97 step/s, accuracy=0.93, loss=0.21, step=27500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 778.75 uttr/s, accuracy=0.84, loss=0.79]\n",
            "Train: 100% 500/500 [02:03<00:00,  4.05 step/s, accuracy=0.94, loss=0.22, step=28000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 776.83 uttr/s, accuracy=0.85, loss=0.77]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 28000, best model saved. (accuracy=0.8516)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:01<00:00,  4.13 step/s, accuracy=0.91, loss=0.30, step=28500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 778.54 uttr/s, accuracy=0.85, loss=0.75]\n",
            "Train: 100% 500/500 [01:58<00:00,  4.20 step/s, accuracy=0.92, loss=0.28, step=29000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 781.90 uttr/s, accuracy=0.85, loss=0.75]\n",
            "Train: 100% 500/500 [02:00<00:00,  4.14 step/s, accuracy=0.90, loss=0.33, step=29500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 799.79 uttr/s, accuracy=0.85, loss=0.73]\n",
            "Train: 100% 500/500 [02:00<00:00,  4.16 step/s, accuracy=0.93, loss=0.22, step=3e+4] \n",
            "Valid: 100% 6912/6944 [00:08<00:00, 819.97 uttr/s, accuracy=0.86, loss=0.72] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 30000, best model saved. (accuracy=0.8552)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:00<00:00,  4.15 step/s, accuracy=0.93, loss=0.20, step=30500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 798.77 uttr/s, accuracy=0.86, loss=0.68]\n",
            "Train: 100% 500/500 [01:56<00:00,  4.28 step/s, accuracy=0.90, loss=0.34, step=31000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 805.03 uttr/s, accuracy=0.86, loss=0.71]\n",
            "Train: 100% 500/500 [01:58<00:00,  4.21 step/s, accuracy=0.94, loss=0.17, step=31500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 806.92 uttr/s, accuracy=0.85, loss=0.72]\n",
            "Train: 100% 500/500 [01:58<00:00,  4.23 step/s, accuracy=0.95, loss=0.19, step=32000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 751.16 uttr/s, accuracy=0.86, loss=0.73]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 32000, best model saved. (accuracy=0.8630)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:34<00:00,  5.28 step/s, accuracy=0.93, loss=0.28, step=32500]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 1000.65 uttr/s, accuracy=0.86, loss=0.72]\n",
            "Train: 100% 500/500 [01:35<00:00,  5.26 step/s, accuracy=0.92, loss=0.24, step=33000]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 998.52 uttr/s, accuracy=0.86, loss=0.72] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.25 step/s, accuracy=0.97, loss=0.13, step=33500]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 990.23 uttr/s, accuracy=0.86, loss=0.69] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.23 step/s, accuracy=0.93, loss=0.27, step=34000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 900.25 uttr/s, accuracy=0.86, loss=0.75] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 34000, best model saved. (accuracy=0.8640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:58<00:00,  4.21 step/s, accuracy=0.96, loss=0.12, step=34500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 784.44 uttr/s, accuracy=0.86, loss=0.67]\n",
            "Train: 100% 500/500 [01:56<00:00,  4.31 step/s, accuracy=0.93, loss=0.22, step=35000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 898.41 uttr/s, accuracy=0.87, loss=0.67] \n",
            "Train: 100% 500/500 [01:59<00:00,  4.18 step/s, accuracy=0.95, loss=0.19, step=35500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 763.32 uttr/s, accuracy=0.86, loss=0.71]\n",
            "Train: 100% 500/500 [01:57<00:00,  4.24 step/s, accuracy=0.93, loss=0.20, step=36000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 772.11 uttr/s, accuracy=0.86, loss=0.70]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 36000, best model saved. (accuracy=0.8675)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:56<00:00,  4.30 step/s, accuracy=0.92, loss=0.27, step=36500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 806.61 uttr/s, accuracy=0.87, loss=0.68]\n",
            "Train: 100% 500/500 [01:56<00:00,  4.29 step/s, accuracy=0.93, loss=0.24, step=37000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 791.28 uttr/s, accuracy=0.86, loss=0.70]\n",
            "Train: 100% 500/500 [01:57<00:00,  4.25 step/s, accuracy=0.96, loss=0.13, step=37500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 802.38 uttr/s, accuracy=0.87, loss=0.69]\n",
            "Train: 100% 500/500 [01:57<00:00,  4.27 step/s, accuracy=0.95, loss=0.17, step=38000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 797.41 uttr/s, accuracy=0.87, loss=0.68]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 38000, best model saved. (accuracy=0.8715)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:57<00:00,  4.25 step/s, accuracy=0.95, loss=0.18, step=38500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 794.66 uttr/s, accuracy=0.86, loss=0.70]\n",
            "Train: 100% 500/500 [01:37<00:00,  5.13 step/s, accuracy=0.95, loss=0.20, step=39000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 948.92 uttr/s, accuracy=0.87, loss=0.69] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.17 step/s, accuracy=0.93, loss=0.24, step=39500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 935.05 uttr/s, accuracy=0.87, loss=0.71] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.16 step/s, accuracy=0.95, loss=0.19, step=4e+4] \n",
            "Valid: 100% 6912/6944 [00:07<00:00, 959.84 uttr/s, accuracy=0.87, loss=0.68] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40000, best model saved. (accuracy=0.8715)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:32<00:00,  5.39 step/s, accuracy=0.96, loss=0.10, step=40500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 968.06 uttr/s, accuracy=0.88, loss=0.65] \n",
            "Train: 100% 500/500 [01:44<00:00,  4.79 step/s, accuracy=0.93, loss=0.23, step=41000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 778.38 uttr/s, accuracy=0.86, loss=0.73]\n",
            "Train: 100% 500/500 [01:36<00:00,  5.17 step/s, accuracy=0.97, loss=0.12, step=41500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 964.24 uttr/s, accuracy=0.87, loss=0.65] \n",
            "Train: 100% 500/500 [01:34<00:00,  5.30 step/s, accuracy=0.96, loss=0.19, step=42000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 962.02 uttr/s, accuracy=0.87, loss=0.68] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 42000, best model saved. (accuracy=0.8777)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:55<00:00,  4.31 step/s, accuracy=0.96, loss=0.10, step=42500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 727.93 uttr/s, accuracy=0.87, loss=0.67]\n",
            "Train: 100% 500/500 [01:58<00:00,  4.22 step/s, accuracy=0.97, loss=0.13, step=43000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 777.33 uttr/s, accuracy=0.87, loss=0.67]\n",
            "Train: 100% 500/500 [01:58<00:00,  4.21 step/s, accuracy=0.96, loss=0.13, step=43500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 711.30 uttr/s, accuracy=0.86, loss=0.72]\n",
            "Train: 100% 500/500 [01:57<00:00,  4.24 step/s, accuracy=0.96, loss=0.12, step=44000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 772.64 uttr/s, accuracy=0.87, loss=0.68]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 44000, best model saved. (accuracy=0.8777)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:56<00:00,  4.29 step/s, accuracy=0.98, loss=0.10, step=44500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 785.89 uttr/s, accuracy=0.88, loss=0.65]\n",
            "Train: 100% 500/500 [01:47<00:00,  4.67 step/s, accuracy=0.96, loss=0.13, step=45000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 983.33 uttr/s, accuracy=0.88, loss=0.64] \n",
            "Train: 100% 500/500 [01:34<00:00,  5.32 step/s, accuracy=0.98, loss=0.07, step=45500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 986.98 uttr/s, accuracy=0.87, loss=0.68] \n",
            "Train: 100% 500/500 [01:33<00:00,  5.34 step/s, accuracy=0.96, loss=0.11, step=46000]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 988.08 uttr/s, accuracy=0.87, loss=0.69] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 46000, best model saved. (accuracy=0.8809)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:32<00:00,  5.40 step/s, accuracy=0.96, loss=0.12, step=46500]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 998.82 uttr/s, accuracy=0.88, loss=0.66] \n",
            "Train: 100% 500/500 [01:34<00:00,  5.30 step/s, accuracy=0.96, loss=0.13, step=47000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 974.64 uttr/s, accuracy=0.87, loss=0.68] \n",
            "Train: 100% 500/500 [01:56<00:00,  4.30 step/s, accuracy=0.97, loss=0.10, step=47500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 791.87 uttr/s, accuracy=0.89, loss=0.63]\n",
            "Train: 100% 500/500 [01:56<00:00,  4.31 step/s, accuracy=0.98, loss=0.07, step=48000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 745.03 uttr/s, accuracy=0.88, loss=0.65]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 48000, best model saved. (accuracy=0.8863)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:36<00:00,  5.17 step/s, accuracy=0.98, loss=0.07, step=48500]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 999.64 uttr/s, accuracy=0.88, loss=0.65] \n",
            "Train: 100% 500/500 [01:38<00:00,  5.05 step/s, accuracy=0.98, loss=0.06, step=49000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 759.55 uttr/s, accuracy=0.88, loss=0.65]\n",
            "Train: 100% 500/500 [01:45<00:00,  4.72 step/s, accuracy=0.97, loss=0.09, step=49500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 980.87 uttr/s, accuracy=0.87, loss=0.66] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.20 step/s, accuracy=0.98, loss=0.10, step=5e+4] \n",
            "Valid: 100% 6912/6944 [00:06<00:00, 995.53 uttr/s, accuracy=0.88, loss=0.64] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 50000, best model saved. (accuracy=0.8863)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:35<00:00,  5.22 step/s, accuracy=0.96, loss=0.11, step=50500]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 988.51 uttr/s, accuracy=0.88, loss=0.66] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.23 step/s, accuracy=0.97, loss=0.07, step=51000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 980.58 uttr/s, accuracy=0.89, loss=0.64] \n",
            "Train: 100% 500/500 [01:40<00:00,  4.97 step/s, accuracy=0.96, loss=0.09, step=51500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 965.80 uttr/s, accuracy=0.88, loss=0.67] \n",
            "Train: 100% 500/500 [01:40<00:00,  5.00 step/s, accuracy=0.97, loss=0.10, step=52000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 954.31 uttr/s, accuracy=0.89, loss=0.63] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 52000, best model saved. (accuracy=0.8869)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:40<00:00,  4.99 step/s, accuracy=0.98, loss=0.09, step=52500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 920.33 uttr/s, accuracy=0.89, loss=0.62]\n",
            "Train: 100% 500/500 [01:43<00:00,  4.82 step/s, accuracy=0.99, loss=0.05, step=53000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 909.05 uttr/s, accuracy=0.89, loss=0.61] \n",
            "Train: 100% 500/500 [01:41<00:00,  4.92 step/s, accuracy=0.99, loss=0.05, step=53500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 944.76 uttr/s, accuracy=0.89, loss=0.62] \n",
            "Train: 100% 500/500 [01:41<00:00,  4.94 step/s, accuracy=0.99, loss=0.06, step=54000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 919.97 uttr/s, accuracy=0.89, loss=0.61] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 54000, best model saved. (accuracy=0.8886)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:44<00:00,  4.77 step/s, accuracy=0.97, loss=0.06, step=54500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 790.13 uttr/s, accuracy=0.89, loss=0.59]\n",
            "Train: 100% 500/500 [01:43<00:00,  4.83 step/s, accuracy=0.97, loss=0.11, step=55000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 906.15 uttr/s, accuracy=0.89, loss=0.60] \n",
            "Train: 100% 500/500 [01:44<00:00,  4.77 step/s, accuracy=0.98, loss=0.05, step=55500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 979.74 uttr/s, accuracy=0.89, loss=0.63] \n",
            "Train: 100% 500/500 [01:42<00:00,  4.86 step/s, accuracy=0.98, loss=0.10, step=56000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 954.43 uttr/s, accuracy=0.89, loss=0.60] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 56000, best model saved. (accuracy=0.8947)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:41<00:00,  4.92 step/s, accuracy=0.98, loss=0.07, step=56500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 962.17 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train: 100% 500/500 [01:42<00:00,  4.89 step/s, accuracy=0.99, loss=0.05, step=57000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 976.68 uttr/s, accuracy=0.89, loss=0.61] \n",
            "Train: 100% 500/500 [01:55<00:00,  4.32 step/s, accuracy=0.99, loss=0.03, step=57500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 735.91 uttr/s, accuracy=0.89, loss=0.63]\n",
            "Train: 100% 500/500 [02:00<00:00,  4.15 step/s, accuracy=0.98, loss=0.05, step=58000]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 793.09 uttr/s, accuracy=0.90, loss=0.58]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 58000, best model saved. (accuracy=0.8971)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:59<00:00,  4.19 step/s, accuracy=0.99, loss=0.03, step=58500]\n",
            "Valid: 100% 6912/6944 [00:11<00:00, 620.96 uttr/s, accuracy=0.89, loss=0.58]\n",
            "Train: 100% 500/500 [02:14<00:00,  3.72 step/s, accuracy=0.98, loss=0.07, step=59000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 668.85 uttr/s, accuracy=0.89, loss=0.65]\n",
            "Train: 100% 500/500 [02:13<00:00,  3.76 step/s, accuracy=0.98, loss=0.06, step=59500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 664.30 uttr/s, accuracy=0.90, loss=0.59]\n",
            "Train: 100% 500/500 [02:09<00:00,  3.87 step/s, accuracy=0.98, loss=0.04, step=6e+4] \n",
            "Valid: 100% 6912/6944 [00:10<00:00, 643.85 uttr/s, accuracy=0.90, loss=0.59]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60000, best model saved. (accuracy=0.8980)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:13<00:00,  3.75 step/s, accuracy=0.96, loss=0.12, step=60500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 684.57 uttr/s, accuracy=0.90, loss=0.58]\n",
            "Train: 100% 500/500 [02:12<00:00,  3.76 step/s, accuracy=0.99, loss=0.03, step=61000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 666.30 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train: 100% 500/500 [02:10<00:00,  3.83 step/s, accuracy=0.99, loss=0.06, step=61500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 689.98 uttr/s, accuracy=0.90, loss=0.57]\n",
            "Train: 100% 500/500 [02:09<00:00,  3.85 step/s, accuracy=0.99, loss=0.04, step=62000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 669.94 uttr/s, accuracy=0.90, loss=0.64]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 62000, best model saved. (accuracy=0.9003)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:13<00:00,  3.76 step/s, accuracy=1.00, loss=0.02, step=62500]\n",
            "Valid: 100% 6912/6944 [00:08<00:00, 804.24 uttr/s, accuracy=0.90, loss=0.61]\n",
            "Train: 100% 500/500 [02:07<00:00,  3.91 step/s, accuracy=0.98, loss=0.04, step=63000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 715.05 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train: 100% 500/500 [02:11<00:00,  3.79 step/s, accuracy=0.98, loss=0.04, step=63500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 645.97 uttr/s, accuracy=0.90, loss=0.61]\n",
            "Train: 100% 500/500 [02:14<00:00,  3.72 step/s, accuracy=0.99, loss=0.03, step=64000]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 701.19 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 64000, best model saved. (accuracy=0.9003)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:12<00:00,  3.77 step/s, accuracy=1.00, loss=0.02, step=64500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 649.12 uttr/s, accuracy=0.90, loss=0.59]\n",
            "Train: 100% 500/500 [02:11<00:00,  3.81 step/s, accuracy=0.99, loss=0.03, step=65000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 671.54 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train: 100% 500/500 [02:23<00:00,  3.50 step/s, accuracy=0.99, loss=0.03, step=65500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 661.37 uttr/s, accuracy=0.90, loss=0.61]\n",
            "Train: 100% 500/500 [02:13<00:00,  3.74 step/s, accuracy=0.99, loss=0.03, step=66000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 651.35 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 66000, best model saved. (accuracy=0.9022)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:14<00:00,  3.72 step/s, accuracy=0.98, loss=0.05, step=66500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 657.27 uttr/s, accuracy=0.90, loss=0.61]\n",
            "Train: 100% 500/500 [02:14<00:00,  3.71 step/s, accuracy=1.00, loss=0.02, step=67000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 632.69 uttr/s, accuracy=0.90, loss=0.58]\n",
            "Train: 100% 500/500 [02:23<00:00,  3.47 step/s, accuracy=0.99, loss=0.03, step=67500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 672.74 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train: 100% 500/500 [02:21<00:00,  3.54 step/s, accuracy=0.99, loss=0.04, step=68000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 650.54 uttr/s, accuracy=0.91, loss=0.56]\n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 68000, best model saved. (accuracy=0.9060)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [02:21<00:00,  3.53 step/s, accuracy=0.99, loss=0.02, step=68500]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 641.81 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train: 100% 500/500 [02:19<00:00,  3.58 step/s, accuracy=0.98, loss=0.05, step=69000]\n",
            "Valid: 100% 6912/6944 [00:10<00:00, 646.41 uttr/s, accuracy=0.90, loss=0.60]\n",
            "Train: 100% 500/500 [02:09<00:00,  3.86 step/s, accuracy=0.99, loss=0.04, step=69500]\n",
            "Valid: 100% 6912/6944 [00:09<00:00, 736.83 uttr/s, accuracy=0.91, loss=0.55]\n",
            "Train: 100% 500/500 [01:44<00:00,  4.80 step/s, accuracy=0.99, loss=0.06, step=7e+4] \n",
            "Valid: 100% 6912/6944 [00:07<00:00, 986.72 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 70000, best model saved. (accuracy=0.9060)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:41<00:00,  4.94 step/s, accuracy=0.98, loss=0.04, step=70500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 939.31 uttr/s, accuracy=0.90, loss=0.56] \n",
            "Train: 100% 500/500 [01:39<00:00,  5.01 step/s, accuracy=1.00, loss=0.01, step=71000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 964.75 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train: 100% 500/500 [01:41<00:00,  4.95 step/s, accuracy=0.99, loss=0.03, step=71500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 937.03 uttr/s, accuracy=0.90, loss=0.59] \n",
            "Train: 100% 500/500 [01:38<00:00,  5.10 step/s, accuracy=1.00, loss=0.01, step=72000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 945.94 uttr/s, accuracy=0.90, loss=0.57] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 72000, best model saved. (accuracy=0.9060)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:37<00:00,  5.12 step/s, accuracy=1.00, loss=0.01, step=72500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 957.22 uttr/s, accuracy=0.90, loss=0.56] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.19 step/s, accuracy=0.99, loss=0.04, step=73000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 975.79 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train: 100% 500/500 [01:34<00:00,  5.27 step/s, accuracy=0.98, loss=0.06, step=73500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 979.14 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.17 step/s, accuracy=0.98, loss=0.04, step=74000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 960.91 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 74000, best model saved. (accuracy=0.9060)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:38<00:00,  5.08 step/s, accuracy=0.99, loss=0.03, step=74500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 985.53 uttr/s, accuracy=0.90, loss=0.56] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.25 step/s, accuracy=0.99, loss=0.03, step=75000]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 987.84 uttr/s, accuracy=0.91, loss=0.56] \n",
            "Train: 100% 500/500 [01:37<00:00,  5.13 step/s, accuracy=1.00, loss=0.01, step=75500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 980.34 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train: 100% 500/500 [01:36<00:00,  5.19 step/s, accuracy=0.98, loss=0.04, step=76000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 976.81 uttr/s, accuracy=0.91, loss=0.54] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 76000, best model saved. (accuracy=0.9097)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:36<00:00,  5.18 step/s, accuracy=0.99, loss=0.02, step=76500]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 1005.22 uttr/s, accuracy=0.91, loss=0.55]\n",
            "Train: 100% 500/500 [01:35<00:00,  5.22 step/s, accuracy=1.00, loss=0.01, step=77000]\n",
            "Valid: 100% 6912/6944 [00:06<00:00, 996.08 uttr/s, accuracy=0.90, loss=0.59] \n",
            "Train: 100% 500/500 [01:35<00:00,  5.26 step/s, accuracy=0.98, loss=0.04, step=77500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 986.16 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train: 100% 500/500 [01:38<00:00,  5.06 step/s, accuracy=0.99, loss=0.03, step=78000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 948.90 uttr/s, accuracy=0.90, loss=0.56] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 78000, best model saved. (accuracy=0.9097)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100% 500/500 [01:41<00:00,  4.92 step/s, accuracy=0.98, loss=0.05, step=78500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 929.60 uttr/s, accuracy=0.91, loss=0.57] \n",
            "Train: 100% 500/500 [01:40<00:00,  4.95 step/s, accuracy=0.99, loss=0.03, step=79000]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 962.68 uttr/s, accuracy=0.90, loss=0.58] \n",
            "Train: 100% 500/500 [01:40<00:00,  4.98 step/s, accuracy=0.98, loss=0.05, step=79500]\n",
            "Valid: 100% 6912/6944 [00:07<00:00, 966.06 uttr/s, accuracy=0.90, loss=0.59] \n",
            "Train: 100% 500/500 [01:40<00:00,  5.00 step/s, accuracy=1.00, loss=0.01, step=8e+4] \n",
            "Valid: 100% 6912/6944 [00:07<00:00, 933.30 uttr/s, accuracy=0.90, loss=0.57] \n",
            "Train:   0% 0/500 [00:00<?, ? step/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80000, best model saved. (accuracy=0.9097)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.multiprocessing as mp\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "  \"\"\"arguments\"\"\"\n",
        "  config = {\n",
        "    \"data_dir\": \"./Dataset\",\n",
        "    \"save_path\": \"model.ckpt\",\n",
        "    \"batch_size\": 256,\n",
        "    \"n_workers\": 0,\n",
        "    \"valid_steps\": 500,\n",
        "    \"warmup_steps\": 200,\n",
        "    \"save_steps\": 2000,\n",
        "    \"total_steps\": 80000,\n",
        "  }\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def main(\n",
        "  data_dir,\n",
        "  save_path,\n",
        "  batch_size,\n",
        "  n_workers,\n",
        "  valid_steps,\n",
        "  warmup_steps,\n",
        "  total_steps,\n",
        "  save_steps,\n",
        "):\n",
        "  \"\"\"Main function.\"\"\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"[Info]: Use {device} now!\")\n",
        "\n",
        "  writer = SummaryWriter('runs/ccc')\n",
        "  writer.add_scalar(\"test/dummy\", 1.0, 0)\n",
        "  writer.flush()\n",
        "\n",
        "  train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n",
        "  train_iterator = iter(train_loader)\n",
        "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
        "\n",
        "  model = Classifier(n_spks=speaker_num).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = AdamW(model.parameters(), lr=1e-3)\n",
        "  scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
        "\n",
        "  best_accuracy = -1.0\n",
        "  best_state_dict = None\n",
        "\n",
        "  pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
        "\n",
        "  for step in range(total_steps):\n",
        "    # Get data\n",
        "    try:\n",
        "      batch = next(train_iterator)\n",
        "    except StopIteration:\n",
        "      train_iterator = iter(train_loader)\n",
        "      batch = next(train_iterator)\n",
        "\n",
        "    loss, accuracy = model_fn(batch, model, criterion, device)\n",
        "    batch_loss = loss.item()\n",
        "    batch_accuracy = accuracy.item()\n",
        "\n",
        "    # Updata model\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    global_step = step + 1\n",
        "    writer.add_scalar(\"train/accuracy\", batch_accuracy, global_step)\n",
        "    writer.flush()\n",
        "\n",
        "    # Log\n",
        "    pbar.update()\n",
        "    pbar.set_postfix(\n",
        "      loss=f\"{batch_loss:.2f}\",\n",
        "      accuracy=f\"{batch_accuracy:.2f}\",\n",
        "      step=step + 1,\n",
        "    )\n",
        "\n",
        "    # Do validation\n",
        "    if (step + 1) % valid_steps == 0:\n",
        "      pbar.close()\n",
        "\n",
        "      valid_accuracy = valid(valid_loader, model, criterion, device)\n",
        "      writer.add_scalar(\"valid/accuracy\", valid_accuracy, global_step)\n",
        "      writer.flush()\n",
        "      \n",
        "      # keep the best model\n",
        "      if valid_accuracy > best_accuracy:\n",
        "        best_accuracy = valid_accuracy\n",
        "        best_state_dict = model.state_dict()\n",
        "\n",
        "      pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
        "\n",
        "    # Save the best model so far.\n",
        "    if (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
        "      torch.save(best_state_dict, save_path)\n",
        "      pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
        "\n",
        "  pbar.close()\n",
        "  writer.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(**parse_args())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R2rx3AyHpQ-"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSuI3WY9Fz78"
      },
      "source": [
        "## Dataset of inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4evns0055Dsx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "  def __init__(self, data_dir):\n",
        "    testdata_path = Path(data_dir) / \"testdata.json\"\n",
        "    metadata = json.load(testdata_path.open())\n",
        "    self.data_dir = data_dir\n",
        "    self.data = metadata[\"utterances\"]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    utterance = self.data[index]\n",
        "    feat_path = utterance[\"feature_path\"]\n",
        "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
        "\n",
        "    return feat_path, mel\n",
        "\n",
        "\n",
        "def inference_collate_batch(batch):\n",
        "  \"\"\"Collate a batch of data.\"\"\"\n",
        "  feat_paths, mels = zip(*batch)\n",
        "\n",
        "  return feat_paths, torch.stack(mels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAinHBG1GIWv"
      },
      "source": [
        "## Main funcrion of Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yQaTt7VDHoRI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Info]: Use cuda now!\n",
            "[Info]: Finish loading data!\n",
            "[Info]: Finish creating model!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_22356\\72179661.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b10059f2c41e4f1cb2f2c3dd689084eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_22356\\2580348096.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mel = torch.load(os.path.join(self.data_dir, feat_path))\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def parse_args():\n",
        "  \"\"\"arguments\"\"\"\n",
        "  config = {\n",
        "    \"data_dir\": \"./Dataset\",\n",
        "    \"model_path\": \"./model.ckpt\",\n",
        "    \"output_path\": \"./output.csv\",\n",
        "  }\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def main(\n",
        "  data_dir,\n",
        "  model_path,\n",
        "  output_path,\n",
        "):\n",
        "  \"\"\"Main function.\"\"\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"[Info]: Use {device} now!\")\n",
        "\n",
        "  mapping_path = Path(data_dir) / \"mapping.json\"\n",
        "  mapping = json.load(mapping_path.open())\n",
        "\n",
        "  dataset = InferenceDataset(data_dir)\n",
        "  dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=inference_collate_batch,\n",
        "  )\n",
        "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
        "\n",
        "  speaker_num = len(mapping[\"id2speaker\"])\n",
        "  model = Classifier(n_spks=speaker_num).to(device)\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model.eval()\n",
        "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
        "\n",
        "  results = [[\"Id\", \"Category\"]]\n",
        "  for feat_paths, mels in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "      mels = mels.to(device)\n",
        "      outs = model(mels)\n",
        "      preds = outs.argmax(1).cpu().numpy()\n",
        "      for feat_path, pred in zip(feat_paths, preds):\n",
        "        results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
        "\n",
        "  with open(output_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(**parse_args())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW04.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "torch-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
